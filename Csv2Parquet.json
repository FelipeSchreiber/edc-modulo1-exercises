{
  "jobConfig": {
    "name": "Csv2Parquet",
    "description": "",
    "role": "arn:aws:iam::689150947157:role/AWSGlueServiceRoleDefault",
    "command": "glueetl",
    "version": "3.0",
    "workerType": "G.1X",
    "numberOfWorkers": 10,
    "maxCapacity": 10,
    "maxRetries": 3,
    "timeout": 2880,
    "maxConcurrentRuns": 1,
    "security": "none",
    "scriptName": "Csv2parquet.py",
    "scriptLocation": "s3://aws-glue-assets-689150947157-us-east-2/scripts/",
    "language": "python-3",
    "jobParameters": [],
    "tags": [],
    "jobMode": "DEVELOPER_MODE",
    "developerMode": true,
    "connectionsList": [],
    "temporaryDirectory": "s3://aws-glue-assets-689150947157-us-east-2/temporary/",
    "logging": true,
    "glueHiveMetastore": true,
    "etlAutoTuning": true,
    "metrics": true,
    "spark": true,
    "bookmark": "job-bookmark-disable",
    "sparkPath": "s3://aws-glue-assets-689150947157-us-east-2/sparkHistoryLogs/",
    "flexExecution": false,
    "minFlexWorkers": null
  },
  "hasBeenSaved": false,
  "script": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n## @params: [JOB_NAME]\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\ndf = spark.read.csv('s3://datalake-felipeschreiber/raw-data/microdados_enem_2020.csv',sep=\";\", header = True)\n(\ndf.write.mode('overwrite')\n        .format('parquet')\n        .partitionBy(\"NU_ANO\")\n        .save('s3://datalake-felipeschreiber/consumer-zone/microdados_enem_2020.parquet')\n)\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\njob.commit()"
}